{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf30fc9-aebc-44a0-95de-50367615dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Sionna imports ---\n",
    "from sionna.phy.utils.plotting import PlotBER\n",
    "from sionna.phy.fec.linear import LinearEncoder\n",
    "from sionna.phy.fec.ldpc import LDPCBPDecoder, LDPC5GEncoder, LDPC5GDecoder\n",
    "from sionna.phy.fec.utils import gm2pcm, load_parity_check_examples\n",
    "\n",
    "# --- Project-specific imports ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gnn import GNN_BP\n",
    "from e2e_model import E2EModel\n",
    "\n",
    "print(f\"All modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c58ffc-55c9-46d7-87ad-ccc85beb58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "if gpus:\n",
    "    try:\n",
    "        gpu_num = 0 # Use GPU 0\n",
    "        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
    "        print('Using GPU number', gpu_num)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8202e5-2920-4bd6-bc77-342a5932a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- LDPC 5G -----\n",
    "params={\n",
    "    # --- Code Parameters ---\n",
    "        \"code\": \"5G-LDPC\",\n",
    "        \"n\": 140,\n",
    "        \"k\": 60,\n",
    "    # --- GNN Architecture ----\n",
    "        \"num_embed_dims\": 16,\n",
    "        \"num_msg_dims\": 16,\n",
    "        \"num_hidden_units\": 48,\n",
    "        \"num_mlp_layers\": 3,\n",
    "        \"num_iter\": 10,\n",
    "        \"reduce_op\": \"sum\",\n",
    "        \"activation\": \"relu\",\n",
    "        \"clip_llr_to\": 20,\n",
    "        \"use_attributes\": False,\n",
    "        \"node_attribute_dims\": 0,\n",
    "        \"msg_attribute_dims\": 0,\n",
    "        \"return_infobits\": False,\n",
    "        \"use_bias\": True,\n",
    "    # --- Training ---- #\n",
    "        \"batch_size\": [128, 128, 128], # bs, iter, lr must have same dim\n",
    "        \"train_iter\": [35000, 300000, 300000],\n",
    "        \"learning_rate\": [5e-4, 1e-4, 1e-5],\n",
    "        \"ebno_db_train\": [2, 8.],\n",
    "        \"ebno_db_eval\": 2.,\n",
    "        \"batch_size_eval\": 1000, # batch size only used for evaluation during training\n",
    "        \"eval_train_steps\": 1000, # evaluate model every N iters\n",
    "    # --- Log ----\n",
    "        \"save_weights_iter\": 10000, # save weights every X iters\n",
    "        \"run_name\": \"LDPC_5G_01\", # name of the stored weights/logs\n",
    "        \"save_dir\": \"results/\", # folder to store results\n",
    "    # --- MC Simulation parameters ----\n",
    "        \"eval_num_iter\": 10, # number of decoding iters to evaluate\n",
    "        \"mc_iters\": 100,\n",
    "        \"mc_batch_size\": 1000,\n",
    "        \"num_target_block_errors\": 500,\n",
    "        \"ebno_db_min\": 0.,\n",
    "        \"ebno_db_max\": 4.5,\n",
    "        \"ebno_db_stepsize\": 0.5,\n",
    "        \"eval_ns\": [140, 280, 420, 280, 280, 280], # evaluate different lengths\n",
    "        \"eval_ks\": [60, 120, 180, 120, 90, 150],\n",
    "        \"sim_esno\": False, # simulate results in EsN0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7e6bf-4337-4518-a04b-692d68f76ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all codes must provide an encoder-layer and a pcm\n",
    "if params[\"code\"]==\"5G-LDPC\":\n",
    "    print(\"Loading 5G NR LDPC code\")\n",
    "\n",
    "    k = params[\"k\"]\n",
    "    n = params[\"n\"]\n",
    "\n",
    "    encoder_5g = LDPC5GEncoder(k, n)\n",
    "    decoder_5g = LDPC5GDecoder(encoder_5g,\n",
    "                               num_iter=params[\"eval_num_iter\"],\n",
    "                               return_infobits=False,\n",
    "                               prune_pcm=True)\n",
    "\n",
    "    pcm, info_bits_map = decoder_5g.get_pcm(True)\n",
    "    pcm = pcm.numpy()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unknown code type\")"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858145a-036d-4e0e-bcac-f8a4062ea851",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2) # we fix the seed to ensure stable convergence\n",
    "\n",
    "# init the GNN decoder\n",
    "gnn_decoder = GNN_BP(pcm=pcm,\n",
    "                     num_embed_dims=params[\"num_embed_dims\"],\n",
    "                     num_msg_dims=params[\"num_msg_dims\"],\n",
    "                     num_hidden_units=params[\"num_hidden_units\"],\n",
    "                     num_mlp_layers=params[\"num_mlp_layers\"],\n",
    "                     num_iter=params[\"num_iter\"],\n",
    "                     reduce_op=params[\"reduce_op\"],\n",
    "                     activation=params[\"activation\"],\n",
    "                     output_all_iter=True,\n",
    "                     clip_llr_to=params[\"clip_llr_to\"],\n",
    "                     use_attributes=params[\"use_attributes\"],\n",
    "                     node_attribute_dims=params[\"node_attribute_dims\"],\n",
    "                     msg_attribute_dims=params[\"msg_attribute_dims\"],\n",
    "                     use_bias=params[\"use_bias\"])\n",
    "\n",
    "e2e_model = E2EModel(encoder=encoder_5g, decoder=gnn_decoder, k=k, n=n, fading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624ebce-9642-4c71-bf10-978e208f1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#  OPTIMIZER & CHECKPOINTS\n",
    "# ===============================\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"learning_rate\"][0])\n",
    "checkpoint_dir = os.path.join(params[\"save_dir\"], params[\"run_name\"], \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(model=gnn_decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "# ===============================\n",
    "#  TRAINING LOOP\n",
    "# ===============================\n",
    "print(\"Starting training...\")\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train_step(batch_size, ebno_db_range):\n",
    "    ebno_db = tf.random.uniform(shape=[batch_size], minval=ebno_db_range[0], maxval=ebno_db_range[1])\n",
    "    with tf.GradientTape() as tape:\n",
    "        b_info, llr_hat, loss = e2e_model(batch_size, ebno_db, training=True)\n",
    "    grads = tape.gradient(loss, e2e_model.decoder.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, e2e_model.decoder.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "for i in range(1, params[\"train_iter\"][0] + 1):\n",
    "    loss = train_step(params[\"batch_size\"][0], params[\"ebno_db_train\"])\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iter: {i}, Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Sionna)",
   "language": "python",
   "name": "sionna_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
